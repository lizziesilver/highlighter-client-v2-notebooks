{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "1. Download ONNX model + config from HL\n",
    "2. Get relevant parameters (e.g. input size, class list) out of config or model file\n",
    "3. ~Download image from HL~\n",
    "4. ~Instantiate ONNX model~\n",
    "4. ~Perform inference on image~\n",
    "5. ~Display results (save to file)~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a9jw3uSeP3H"
   },
   "source": [
    "# House Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_am_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "if i_am_running_in_colab():\n",
    "    %env PYPI_USERNAME=\"...\"\n",
    "    %env PYPI_PASSWORD=\"...\"\n",
    "    !git clone https://github.com/tall-josh/highlighter-client-v2-notebooks.git\n",
    "    !bash highlighter-client-v2-notebooks/colab-scripts/setup-train-mmdet.sh\n",
    "    !mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(mmdet.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5xgoT06YVuG"
   },
   "source": [
    "# Create a HLClient object from credentials\n",
    "\n",
    "This client will be used when we need to communicate with Highlighter via GraphQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HL_WEB_GRAPHQL_API_TOKEN=\"...\"\n",
    "HL_WEB_GRAPHQL_ENDPOINT=\"https://<account-name>.highlighter.ai/graphql\"\n",
    "img_id = 5568090\n",
    "data_dir='data'\n",
    "\n",
    "from highlighter_client.gql_client import HLClient\n",
    "from highlighter_client.io import multithread_graphql_image_download\n",
    "\n",
    "# Needed when using HighlighterClient in a notebook environment\n",
    "HLClient._async = 'AUTO'\n",
    "client = HLClient.from_credential(api_token=HL_WEB_GRAPHQL_API_TOKEN,\n",
    "                                  endpoint_url=HL_WEB_GRAPHQL_ENDPOINT)\n",
    "\n",
    "# download image for inference\n",
    "multithread_graphql_image_download(client, \n",
    "                                   image_ids=[img_id], \n",
    "                                   image_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ONNX model and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_file = f'{data_dir}/insulator_tie_model_epoch_15.onnx'\n",
    "config_file = f'{data_dir}/fixed-model-config.py'\n",
    "img_file = f'{data_dir}/{str(img_id)}.jpg'\n",
    "output_inferences_file = f'{data_dir}/{str(img_id)}_with_inferences.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from mmcv import Config\n",
    "from mmdet.core.export import preprocess_example_input\n",
    "from mmdet.core.export.model_wrappers import ONNXRuntimeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_config = Config.fromfile(config_file)\n",
    "\n",
    "# mystery: where do the 800 and 1216 come from?\n",
    "# they're not the image scale in the config - that's (2400, 3200)\n",
    "input_config = {'input_shape': (1,3,\n",
    "                                800, # onnx_config.scales[0]['image_size'][0], \n",
    "                                1216 ), #onnx_config.scales[0]['image_size'][1]),\n",
    "                'input_path': img_file,\n",
    "                'normalize_cfg':onnx_config.img_norm_cfg,}\n",
    "\n",
    "one_img, one_meta = preprocess_example_input(input_config)\n",
    "img_list, img_meta_list = [one_img], [[one_meta]]\n",
    "img_list = [_.cuda().contiguous() for _ in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery: the first time you run this cell it appears to hang. You have to\n",
    "# interrupt the kernel and rerun it, at which point it will finish fast\n",
    "onnx_model = ONNXRuntimeDetector(onnx_model_file, \n",
    "                                 class_names=np.array(['Insulator', 'Tie Wire']), \n",
    "                                 device_id=0)\n",
    "\n",
    "onnx_results = onnx_model(img_list, img_metas=img_meta_list, return_loss=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img = one_meta['show_img']\n",
    "score_thr=0.5\n",
    "onnx_model.show_result(\n",
    "            show_img,\n",
    "            onnx_results,\n",
    "            score_thr=score_thr,\n",
    "            show=True,\n",
    "            win_name='ONNXRuntime',\n",
    "            out_file=output_inferences_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_mmdetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

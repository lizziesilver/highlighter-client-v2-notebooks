{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "1. Download ONNX model + config from HL\n",
    "2. Get relevant parameters (e.g. input size, class list) out of config or model file\n",
    "3. Download image from HL\n",
    "4. ~Instantiate ONNX model~\n",
    "4. ~Perform inference on image~\n",
    "5. ~Display results (save to file)~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a9jw3uSeP3H"
   },
   "source": [
    "# House Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(mmdet.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ONNX model and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_file = \"insulator_tie_model_epoch_15.onnx\"\n",
    "config_file = \"fixed-model-config.py\"\n",
    "img_file = \"/mnt/scratch/experiments/images/5568090.jpg\"\n",
    "img_file_2 = '/mnt/scratch/experiments/images/5567791.jpg'\n",
    "output_inferences_file = 'show_inferences.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from mmcv import Config\n",
    "from mmdet.core.export import preprocess_example_input\n",
    "from mmdet.core.export.model_wrappers import ONNXRuntimeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_config = Config.fromfile(config_file)\n",
    "\n",
    "# mystery: where do the 800 and 1216 come from?\n",
    "# they're not the image scale in the config - that's (2400, 3200)\n",
    "input_config = {'input_shape': (1,3,\n",
    "                                800, # onnx_config.scales[0]['image_size'][0], \n",
    "                                1216 ), #onnx_config.scales[0]['image_size'][1]),\n",
    "                'input_path': img_file,\n",
    "                'normalize_cfg':onnx_config.img_norm_cfg,}\n",
    "\n",
    "one_img, one_meta = preprocess_example_input(input_config)\n",
    "img_list, img_meta_list = [one_img], [[one_meta]]\n",
    "img_list = [_.cuda().contiguous() for _ in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery: the first time you run this cell it appears to hang. You have to\n",
    "# interrupt the kernel and rerun it, at which point it will finish fast\n",
    "onnx_model = ONNXRuntimeDetector(onnx_model_file, \n",
    "                                 class_names=np.array(['Insulator', 'Tie Wire']), \n",
    "                                 device_id=0)\n",
    "\n",
    "onnx_results = onnx_model(img_list, img_metas=img_meta_list, return_loss=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img = one_meta['show_img']\n",
    "score_thr=0.5\n",
    "onnx_model.show_result(\n",
    "            show_img,\n",
    "            onnx_results,\n",
    "            score_thr=score_thr,\n",
    "            show=True,\n",
    "            win_name='ONNXRuntime',\n",
    "            out_file=output_inferences_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5xgoT06YVuG"
   },
   "source": [
    "# Create a HLClient object from credentials\n",
    "\n",
    "This client will be used when we need to communicate with Highlighter via GraphQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHrsxPvHyk7g"
   },
   "outputs": [],
   "source": [
    "HL_WEB_GRAPHQL_API_TOKEN=\"...\"\n",
    "HL_WEB_GRAPHQL_ENDPOINT=\"https://<ACCOUNT_NAME>.highlighter.ai/graphql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0UUUeuy_mMb"
   },
   "outputs": [],
   "source": [
    "from highlighter_client.gql_client import HLClient\n",
    "\n",
    "# Needed when using HighlighterClient in a notebook environment\n",
    "HLClient._async = True\n",
    "\n",
    "# Small helper function for displaying the DataFrames in the highlighter clinet\n",
    "# dataset object\n",
    "def display_ds(ds, count=10):\n",
    "    display(ds.annotations_df.head(count))\n",
    "    display(ds.images_df.head(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_jUmZFgYUD3"
   },
   "outputs": [],
   "source": [
    "client = HLClient.from_credential(api_token=HL_WEB_GRAPHQL_API_TOKEN, endpoint_url=HL_WEB_GRAPHQL_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeToktEKYtYC"
   },
   "source": [
    "# Read Dataset from Highlighter\n",
    "\n",
    "`HighlighterClient` represents datasets as two Pandas DataFrames `annotations_df` and `images_df`. We can populate a `HighlighterClient.Dataset` in several ways using `Readers`. You can list the availaible `Readers` and load one from its name. In this case we'll be loading the `HighlighterSubmissionsReader` so we can pull submissions down from Highlighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yf-O1fQTs4kG"
   },
   "outputs": [],
   "source": [
    "from highlighter_client.datasets import get_reader, READERS\n",
    "\n",
    "print(f\"READERS: {list(READERS.keys())}\")\n",
    "\n",
    "reader = get_reader(\"highlighter_submissions\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBVvkpRSaa7N"
   },
   "outputs": [],
   "source": [
    "# View the doc string and function signature\n",
    "# Note it expects a submissions generator\n",
    "# We will create one in a moment.\n",
    "?reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JR0IfBTban8"
   },
   "source": [
    "Once we have a `Reader` we can initialize a `highlighter_client.Dataset` object \n",
    "and with that `Reader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Q1bTTxbzDQL"
   },
   "outputs": [],
   "source": [
    "from highlighter_client.datasets.dataset import Dataset\n",
    "ds = Dataset(reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQi6oDCTcQR0"
   },
   "source": [
    "Now we have a `highlighter_client.Dataset` with a `HighlighterSubmissionsReader` we can populate our `DataFrames`.\n",
    "\n",
    "To understand this we need to know two things.\n",
    "\n",
    "1. `highlighter_client` uses Pandas `BaseModel` to tell GraphQL what values to return from a query. Some common `BaseModel`s are defined in `highlighter_client.base_models` but if you want more fine grained control you can define your own.\n",
    "\n",
    "2. Some GraphQL queries may return many results. These types of queries are called `Connections` are are named accordingly in the code. There is a `paginate` function that takes a `Connection` query and returns a Python Generator.\n",
    "\n",
    "For more information on the BaseModels see `highlighter_client/base_models.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKjdi2zzgUGX"
   },
   "outputs": [],
   "source": [
    "from highlighter_client.base_models import DatasetSubmissionTypeConnection\n",
    "from highlighter_client.paginate import paginate\n",
    "\n",
    "dataset_id = ?\n",
    "\n",
    "submissions_gen = paginate(\n",
    "client.datasetSubmissionConnection,\n",
    "DatasetSubmissionTypeConnection,\n",
    "datasetId=dataset_id,\n",
    ")\n",
    "\n",
    "ds.read(submissions_gen=submissions_gen)\n",
    "display_ds(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_mmdetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

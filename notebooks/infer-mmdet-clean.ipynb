{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "1. ~Download ONNX model from HL~\n",
    "2. Make sure the relevant config is packaged up with the model\n",
    "2. Get relevant parameters (e.g. input size, class list) out of config or model file\n",
    "3. ~Download image from HL~\n",
    "4. ~Instantiate ONNX model~\n",
    "4. ~Perform inference on image~\n",
    "5. ~Display results (save to file)~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a9jw3uSeP3H"
   },
   "source": [
    "# House Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_am_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "if i_am_running_in_colab():\n",
    "    %env PYPI_USERNAME=\"...\"\n",
    "    %env PYPI_PASSWORD=\"...\"\n",
    "    !git clone https://github.com/tall-josh/highlighter-client-v2-notebooks.git\n",
    "    !bash highlighter-client-v2-notebooks/colab-scripts/setup-infer-mmdet.sh\n",
    "    !mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5xgoT06YVuG"
   },
   "source": [
    "# Create a HLClient object from credentials, download data\n",
    "\n",
    "This client will be used when we need to communicate with Highlighter via GraphQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HL_WEB_GRAPHQL_API_TOKEN=\"...\"\n",
    "HL_WEB_GRAPHQL_ENDPOINT=\"https://<account-name>.highlighter.ai/graphql\"\n",
    "training_run_id = 285\n",
    "tr_tarball_path = Path(f\"{data_dir}/training_run_{str(training_run_id)}.tar.gz\")\n",
    "\n",
    "from highlighter_client.gql_client import HLClient\n",
    "from highlighter_client.io import multithread_graphql_image_download, download_bytes\n",
    "from highlighter_client.base_models import TrainingRunType\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "# Needed when using HighlighterClient in a notebook environment\n",
    "HLClient._async = 'AUTO'\n",
    "client = HLClient.from_credential(api_token=HL_WEB_GRAPHQL_API_TOKEN,\n",
    "                                  endpoint_url=HL_WEB_GRAPHQL_ENDPOINT)\n",
    "\n",
    "# download image for inference\n",
    "multithread_graphql_image_download(client, \n",
    "                                   image_ids=[img_id], \n",
    "                                   image_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download training run archive\n",
    "result = client.trainingRun(\n",
    "        return_type=TrainingRunType,\n",
    "        id=training_run_id\n",
    "        )\n",
    "\n",
    "result_str = yaml.dump(result.dict())\n",
    "print(f\"{result_str}\")\n",
    "\n",
    "download_bytes(\n",
    "        result.modelImplementationFileUrl,\n",
    "        save_path=tr_tarball_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the onnx file and model manifest\n",
    "tr_dir = f'{data_dir}/training_run_{str(training_run_id)}'\n",
    "with tarfile.open(str(tr_tarball_path)) as storage:\n",
    "        storage.extractall(str(tr_dir))\n",
    "\n",
    "# read the image normalisation config from the model manifest\n",
    "with open(f'{tr_dir}/manifest.yaml') as m:\n",
    "    manifest = yaml.safe_load(m)\n",
    "norm_cfg = [stage for stage in manifest['other']['pre_proc_conf']['pipeline'] \n",
    "            if stage['type'] == 'Normalize'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ONNX model and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_file = f'{tr_dir}/insulator_tie_model_epoch_15.onnx'\n",
    "img_file = f'{data_dir}/{str(img_id)}.jpg'\n",
    "output_inferences_file = f'{data_dir}/{str(img_id)}_with_inferences.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from mmdet.core.export import preprocess_example_input\n",
    "from mmdet.core.export.model_wrappers import ONNXRuntimeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery: where do the 800 and 1216 come from?\n",
    "# they're not the image scale in the config - that's (2400, 3200)\n",
    "input_config = {'input_shape': (1,3,800,1216),\n",
    "                'input_path': img_file,\n",
    "                'normalize_cfg':norm_cfg,}\n",
    "\n",
    "one_img, one_meta = preprocess_example_input(input_config)\n",
    "img_list, img_meta_list = [one_img], [[one_meta]]\n",
    "img_list = [_.cuda().contiguous() for _ in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery: the first time you run this cell it appears to hang. You have to\n",
    "# interrupt the kernel and rerun it, at which point it will finish fast\n",
    "onnx_model = ONNXRuntimeDetector(onnx_model_file, \n",
    "                                 class_names=np.array(['Insulator', 'Tie Wire']), \n",
    "                                 device_id=0)\n",
    "\n",
    "onnx_results = onnx_model(img_list, img_metas=img_meta_list, return_loss=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img = one_meta['show_img']\n",
    "score_thr=0.5\n",
    "onnx_model.show_result(\n",
    "            show_img,\n",
    "            onnx_results,\n",
    "            score_thr=score_thr,\n",
    "            show=True,\n",
    "            win_name='ONNXRuntime',\n",
    "            out_file=output_inferences_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_mmdetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
